# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This dataset contains bank marketing campaigns data based on the phone calls to potential clients. The target was to convince the potential clients to make a term deposit at the bank. In this project, we try predict whether the potential client would accept a deposit.

The best performing model was the AutoML, which uses classification find the best hyperparameters.
1. Accuracy for Scikit-learn: 0.9143
2. Accuracy for AutoML pipeline (VotingEnsemble): 0.9159

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

Dataset is here: https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv . 
Data Cleaning and Prep: Fucntion clean_data is used to clean the data of missing values; a data dict is added to convert the categorical fields to numeric; then we one hot encode the data. After that, we split the data into 85:15 ratio using the train_test_split function. 
Algorithm: Logistic Regression. 
Hyperparameters with their Search Spaces:

1. C: The inverse of the reqularization strength. '--C' : choice(0.001,0.01,0.1,1,10,20,50,100,200,500,1000),
2. max_iter: Maximum number of iterations to converge.  '--max_iter': choice(50,100,300)

Configuration that defines a HyperDrive run:

  - estimator: It's the estimator that will be called with sampled hyper parameters from the chosen dataset.
  - run_config: It's the object for setting up configuration for script/notebook runs.
  - pipeline: It's the pipeline object for setting up configuration for pipeline runs. The pipeline object will be called with the sample hyperparameters to submit pipeline runs. 

Note: We need to specify only one of the following parameters: estimator, run_config, or pipeline.
  
**What are the benefits of the parameter sampler you chose?**

RandomParameterSampling is used here instead of GridParameterSampling since, the hyperparameters are randomly selected from the search space and it supports early termination of low-performance runs.

**What are the benefits of the early stopping policy you chose?**

BanditPolicy is used here which is an "aggressive" early stopping policy. It is preferrable because it prevents more runs compared to more conservative policies and saves more computational time. It is an early actor.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

In AutoML many pipelines are produced simultaneously that run different algorithms and parameters in an automated way. Parameters used to setup AutoML train:

experiment_timeout_minutes = 30

task ='classification' : We'll use "classification" since we are trying to see if the client would accept to make a term deposit with the bank.

compute_target : The compute target.

training_data : The data for model training.

label_column_name : Contains labels of the train data.

n_cross_validations=3 : Number of cross validations to perform when user validation data is not specified.

primary_metric = 'accuracy' : The metric AML will optimize for model selection.

enable_early_stopping = True : choice to enable early termination policy. 

The best model from AutoML was the VotingEnsemble which came from iteration 29 with an accuracy of 0.9159. 

The hypermeters for the model were:
'C': '10'
'max_iter':'50'
min_samples_split=0.10368421052631578,
min_weight_fraction_leaf=0.0,
n_estimators=50,
n_jobs=1,
oob_score=False,
random_state=None,
verbose=0,
warm_start=False

## Pipeline Comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?

The two models had differences in terms of accuracy. The individual model had an accuracy of 0.9143 and the AutoML modelhad an accuracy of 0.9159. The AutoML was better than the individual one. In terms of architecture,
the AutoML used classification whereas the individual model used Scikit-learn model. 

Although there isn't much a difference in Accuracy between the two models, AutoML helps us more by showing the importance of each feature for prediction and also shows some useful metric outputs like:
  - weighted_accuracy: 0.95518
  - f1_score_weighted: 0.91307
  - precision_score_macro: 0.79556

## Future Work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

  - Use a different less aggressive stopping policy
  - Use Gridsampling



